# 关于 Starbucks_Capstone_Project 的一点分析思路和思考

## 项目背景
本数据集是基于星巴克真实数据集，模拟 Starbucks rewards 移动 app 上用户行为的数据。每隔几天，星巴克会向 app 的用户发送一些推送。这个推送可能仅仅是一条饮品的广告或者是折扣券或 BOGO（买一送一）。不同顾客收到的推送可能是不同的，种类可能不同，数量可能不同，甚至存在没有收到推送的顾客。
我们会将交易数据、人口统计数据和推送数据结合起来判断不同人群面对推送会做出怎样的反应，不同的推送是否会影响购买行为。如果有，那么是怎样的影响。

## 如何去处理数据集，并建立分析思路
为了探索推送和购买行为之间的关系，可以依据数据，建立模型去做出评估和预测。

先来看看有哪些数据，这些数据是不是clean，是否需要补充、删减、组合，甚至是否需要修改（当然修改数据是非常谨慎的，这次项目没有去修改数据^_^）。

对于顾客数据集（profile），某些字段是存在空值的，对于空值的处理，是需要一定判断的，看看数据分布，如果空值不多，而且影响了分布，那就可以删除这些空值存在的行。顾客数据集中有年龄、性别、收入和入会时间，这些信息很有价值，可能是影响购买的重要feature。

对于推送数据集（portfolio），展示了有哪些推送类型，用什么推送方式，推送持续了多久，优惠力度，都会影响购买。

第三个就是事件数据集（transcript），记录了每一次交易和推送。我组合了推送、顾客和时间的数据集，这样每一次交易都是拥有很多feature的，诸如顾客信息，推送状况，交易状况的指标维度。基于交易漏斗思维，每一次交易最终会实现转换，是否会成功，所以我对每一次交易也添加了是否成功的标签，这个标签就是每一次交易的target。

当我拥有了feature和target，很自然就可以建立一个机器学习模型，根据大量数据去做评估和预测。机器学习模型有很多种，我选取了sklearn中的常用模型：logistic regression, random forest, and gradient boosting models。其中，random forest是训练数据集中的最佳模型，其准确性为0.75，f1_score为0.75，Precision为0.722，Recall为0.775。当我们使用测试数据进行分析时，准确性达到0.69，f1_score达到0.69，可以说该模型没有出现预测的过拟合。

## 分析成果
可以看出高质量推送具有的特征是，
- 需要有discount的
- 需要持续一次时间的
- reward并不需要很高，这点讨巧。

而哪些feature会影响购买行为呢？从测试数据集可以发现，
- 收入是重要因素
- 顾客对discount的反应要比bogo的要好。
- 男性比女性更有可能购买。
- 年龄段20-30顾客更容易消费。
- 通过社交平台进行推广，效果更好。

这些分析过程和成果在jupyter notebook中都有展示，选用python，使用的环境是常见库，当然整个项目最关键的地方就是如何实现交易转换漏斗，如何用机器学习模型去解释和预测现实中的购买行为。进一步改善模型目前有两个思路：
- 由于客户收入、报价持续时间、报价奖励这些数字特征对于模型影响较大：可以创建一个融合以上特征的全新多项式指标来改善机器学习模型。如new_feature特征，`new_feature = duration*reward/income`，之后将该特征也做归一化处理，进行模型训练。
- 是不是可以尝试用主成分分析经常用于减少数据集的维数。

## 跳脱项目外的一些思考
Udacity提供的数据集是非常棒的，已经有了很多维度，但是生活经验告诉我们，影响销量的已经不仅仅是这款产品自身做的好不好，而是要放眼其相应行业，甚至相关行业。国人不仅仅由于推送的变化去改变自己喝咖啡的次数，他可能为了和朋友逛街而使用了喜茶推送，他可能由于工作太忙而选择了瑞幸的配送，他可能由于当前的新冠肺炎疫情（2020年2月，中国加油）宅在家而选择了已经囤的其他饮料。

**所以要跳脱数据，看看更精彩的大环境。**


##### 文档说明
- 在Jupyter notebook文件`Starbucks_Capstone_notebook-zh.ipynb`中主要包含两部分内容，
    1. 数据读取、清洗、合并
    2. 机器学习模型的建立与评估
- 需要读取的原始数据，即在 ./data 路径下的.json 文件

##### 成果
1. 机器学习模型的输出
即 ./models_0219 路径下的.joblib文件
2. 清洗后数据集的输出
将 DataFrame中 clean_data 输出为 .db 数据库文件